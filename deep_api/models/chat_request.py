# coding: utf-8

"""
    DeepAPI

    Our AI API service offers a seamless and efficient solution for businesses and individuals seeking to leverage the power of artificial intelligence without the complexity and cost of managing their infrastructure. By choosing DeepAPI, you eliminate the need to invest in and maintain expensive servers and GPUs, as we provide a robust, scalable, cloud-based platform operating out of multiple data centers that ensures high performance, low latency and reliability. Our on-demand service allows you to pay only for what you use, ensuring a cost-effective approach to accessing cutting-edge AI capabilities. Get your API key today at https://deepapi.ai

    The version of the OpenAPI document: 1.0.0
    Contact: contact@deepapi.ai
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, Field, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from deep_api.models.chat_request_chats_inner_inner import ChatRequestChatsInnerInner
from typing import Optional, Set
from typing_extensions import Self

class ChatRequest(BaseModel):
    """
    ChatRequest
    """ # noqa: E501
    model: Optional[StrictStr] = Field(default=None, description="The model to use. Currently only llama-2-7b-chat is supported, however, more models are planned. Stay tuned! ðŸ‘€")
    chats: Optional[Annotated[List[Annotated[List[ChatRequestChatsInnerInner], Field(max_length=100)]], Field(max_length=100)]] = Field(default=None, description="An array of chats, where each chat may have multiple messages. To optimize efficiency, it's recommended that you batch these chats into a single request whenever possible. Additionally, when structuring these conversations, it's required that the first and last messages are from the user and that they alternate between the user and the assistant.")
    temperature: Optional[Union[Annotated[float, Field(le=1.0, strict=True, ge=0.1)], Annotated[int, Field(le=1, strict=True, ge=1)]]] = Field(default=0.6, description="This parameter controls the level of randomness and creativity in the generated text. A higher temperature value will produce more unique and unpredictable responses, while a lower temperature will produce more repetitive and predictable responses. The default temperature value is set to 0.6. However, depending on your specific use case, you may need to experiment with different temperature values. For instance, a higher temperature value might be more appropriate if you're looking for more creative and diverse responses. On the other hand, if you require more structured and predictable responses, a lower temperature value may be more suitable.")
    __properties: ClassVar[List[str]] = ["model", "chats", "temperature"]

    @field_validator('model')
    def model_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['llama-2-7b-chat']):
            raise ValueError("must be one of enum values ('llama-2-7b-chat')")
        return value

    model_config = {
        "populate_by_name": True,
        "validate_assignment": True,
        "protected_namespaces": (),
    }


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ChatRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in chats (list of list)
        _items = []
        if self.chats:
            for _item in self.chats:
                if _item:
                    _items.append(
                         [_inner_item.to_dict() for _inner_item in _item if _inner_item is not None]
                    )
            _dict['chats'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ChatRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "model": obj.get("model"),
            "chats": [
                    [ChatRequestChatsInnerInner.from_dict(_inner_item) for _inner_item in _item]
                    for _item in obj["chats"]
                ] if obj.get("chats") is not None else None,
            "temperature": obj.get("temperature") if obj.get("temperature") is not None else 0.6
        })
        return _obj


